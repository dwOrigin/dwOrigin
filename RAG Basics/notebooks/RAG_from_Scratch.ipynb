{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **RAG from Scratch**\n",
        "\n",
        "Authored by [Kalyan KS](https://www.linkedin.com/in/kalyanksnlp/). To stay updated with LLM, RAG and Agent updates, you can follow me on [Twitter](https://x.com/kalyan_kpl).\n",
        "\n",
        "- Step-1 : Extract text\n",
        "- Step-2 : Chunk the extracted text\n",
        "- Step-3 : Create a vector store with the chunks\n",
        "- Step-4 : Create a retriever which returns the relevant chunks\n",
        "- Step-5 : Build context from the relevant chunk texts\n",
        "- Step-6 : Build the RAG pipeline\n",
        "- Step-7 : Run the RAG pipeline to get the answer."
      ],
      "metadata": {
        "id": "hBldBXEn-s2G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Install libraries**"
      ],
      "metadata": {
        "id": "ZExw2kIQ-vIu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "VeQwqlMC9mOE",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -qU PyPDF2 chromadb litellm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Set up the LLM API Key**"
      ],
      "metadata": {
        "id": "JGPGrBwB_UfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get(\"OPENAI_API_KEY\")\n"
      ],
      "metadata": {
        "id": "59BAtNsw_WX0"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Extract Text**"
      ],
      "metadata": {
        "id": "BItWSfV5_YEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader"
      ],
      "metadata": {
        "id": "wl1GlnRz_kv3"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "def text_extract(pdf_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts text from all pages of a given PDF file.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "        str: Extracted text from the PDF, concatenated with newline separators.\n",
        "    \"\"\"\n",
        "\n",
        "    # An empty list to store extracted text from PDF pages\n",
        "    pdf_pages = []\n",
        "\n",
        "    # Open the PDF file in binary read mode\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "\n",
        "        # Create a PdfReader object to read the PDF\n",
        "        pdf_reader = PdfReader(file)\n",
        "\n",
        "        # Iterate through all pages in the PDF\n",
        "        for page in pdf_reader.pages:\n",
        "\n",
        "            # Extract text from the current page\n",
        "            text = page.extract_text()\n",
        "\n",
        "            # Append the extracted text to the list\n",
        "            pdf_pages.append(text)\n",
        "\n",
        "    # Join all extracted text using newline separator\n",
        "    pdf_text = \"\\n\".join(pdf_pages)\n",
        "\n",
        "    # Return the extracted text as a single string\n",
        "    return pdf_text\n"
      ],
      "metadata": {
        "id": "j9ud4GFX_fMz"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the PDF file\n",
        "import requests\n",
        "\n",
        "pdf_url = 'https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf'\n",
        "response = requests.get(pdf_url)\n",
        "\n",
        "pdf_path = 'attention_is_all_you_need.pdf'\n",
        "with open(pdf_path, 'wb') as file:\n",
        "    file.write(response.content)"
      ],
      "metadata": {
        "id": "ttlt3RYWAh8n"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_text = text_extract(pdf_path)"
      ],
      "metadata": {
        "id": "_NK7uS8aAwxC"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pdf_text[:300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIuE4C3WBCw7",
        "outputId": "3f74c457-bbea-4192-f698-63534785ff56"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Is All You Need\n",
            "Ashish Vaswani\u0003\n",
            "Google Brain\n",
            "avaswani@google.comNoam Shazeer\u0003\n",
            "Google Brain\n",
            "noam@google.comNiki Parmar\u0003\n",
            "Google Research\n",
            "nikip@google.comJakob Uszkoreit\u0003\n",
            "Google Research\n",
            "usz@google.com\n",
            "Llion Jones\u0003\n",
            "Google Research\n",
            "llion@google.comAidan N. Gomez\u0003y\n",
            "University of Toronto\n",
            "aidan@c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Chunk Text**\n",
        "\n"
      ],
      "metadata": {
        "id": "6gFK0HfYBb5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import re\n",
        "from collections import deque\n",
        "\n",
        "\n",
        "def text_chunk(text: str, max_length: int = 1000) -> List[str]:\n",
        "    \"\"\"\n",
        "    Splits a given text into chunks while ensuring that sentences remain intact.\n",
        "\n",
        "    The function maintains sentence boundaries by splitting based on punctuation\n",
        "    (. ! ?) and attempts to fit as many sentences as possible within `max_length`\n",
        "    per chunk.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be chunked.\n",
        "        每个块最大的长度是1000\n",
        "        max_length (int, optional): Maximum length of each chunk. Default is 1000.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of text chunks, each containing full sentences.\n",
        "    \"\"\"\n",
        "\n",
        "    # Split text into sentences while ensuring punctuation (. ! ?) stays at the end\n",
        "    sentences = deque(re.split(r'(?<=[.!?])\\s+', text.replace('\\n', ' ')))\n",
        "\n",
        "    # An empty list to store the final chunks\n",
        "    chunks = []\n",
        "\n",
        "    # Temporary string to hold the current chunk\n",
        "    chunk_text = \"\"\n",
        "\n",
        "    while sentences:\n",
        "        # Access sentence from the deque and strip any extra spaces\n",
        "        sentence = sentences.popleft().strip()\n",
        "\n",
        "        # Check if the sentence is non-empty before processing\n",
        "        if sentence:\n",
        "            # If adding this sentence exceeds max_length and chunk_text is not empty, store the current chunk\n",
        "            # 每个语块大小为1000，如果当前chunk和sentence的长度大于1000，且chunk的长度不为0时，会将其进行存储到一个语块中\n",
        "            if len(chunk_text) + len(sentence) > max_length and chunk_text:\n",
        "\n",
        "                # Save the current chunk\n",
        "                chunks.append(chunk_text)\n",
        "\n",
        "                # Start a new chunk with the current sentence\n",
        "                chunk_text = sentence\n",
        "            else:\n",
        "                # Append the sentence to the current chunk with a space\n",
        "                chunk_text += \" \" + sentence\n",
        "\n",
        "    # Add the last chunk if there's any remaining text\n",
        "    if chunk_text:\n",
        "        chunks.append(chunk_text)\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "wsYaJ2xAQ-9D"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = text_chunk(pdf_text)"
      ],
      "metadata": {
        "id": "stO6tscmToxv"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of chunks ={len(chunks)}\")\n",
        "print(chunks[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrPVFAyXTsaQ",
        "outputId": "c3fb3b2e-5f3a-436b-cb73-083ab5f8d1da"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks =36\n",
            "Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. 1 Introduction Recurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks in particular, have been ﬁrmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [ 29,2,5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13]. \u0003Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create the Vector Store**"
      ],
      "metadata": {
        "id": "zyfi5blWT-cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up Chromadb\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "from chromadb.api.models import Collection\n",
        "\n",
        "def create_vector_store(db_path: str) -> Collection:\n",
        "    \"\"\"\n",
        "    Creates a persistent ChromaDB vector store with OpenAI embeddings.\n",
        "\n",
        "    Args:\n",
        "        db_path (str): Path where the ChromaDB database will be stored.\n",
        "\n",
        "    Returns:\n",
        "        Collection: A ChromaDB collection object for storing and retrieving embedded vectors.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize a ChromaDB PersistentClient with the specified database path\n",
        "    client = chromadb.PersistentClient(path=db_path)\n",
        "\n",
        "    # Create an embedding function using OpenAI's text embedding model\n",
        "    embeddings = embedding_functions.OpenAIEmbeddingFunction(\n",
        "        api_key=userdata.get(\"OPENAI_API_KEY\"),  # Retrieve API key from user data\n",
        "        model_name=\"text-embedding-3-small\"  # Specify the embedding model\n",
        "    )\n",
        "\n",
        "    # Create a new collection in the ChromaDB database with the embedding function\n",
        "    db = client.create_collection(\n",
        "        name=\"pdf_chunks\",  # Name of the collection where embeddings will be stored\n",
        "        embedding_function=embeddings  # Apply the embedding function\n",
        "    )\n",
        "\n",
        "    # Return the created ChromaDB collection\n",
        "    return db\n"
      ],
      "metadata": {
        "id": "G36-SWrFUANV"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert chunks into vector store\n",
        "import os\n",
        "import uuid\n",
        "\n",
        "def insert_chunks_vectordb(chunks: List[str], db: Collection, file_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Inserts text chunks into a ChromaDB vector store with metadata.\n",
        "\n",
        "    Args:\n",
        "        chunks (List[str]): List of text chunks to be stored.\n",
        "        db (Collection): The ChromaDB collection where the chunks will be inserted.\n",
        "        file_path (str): Path of the source file for metadata.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract the file name from the given file path\n",
        "    file_name = os.path.basename(file_path)\n",
        "\n",
        "    # Generate unique IDs for each chunk\n",
        "    id_list = [str(uuid.uuid4()) for _ in range(len(chunks))]\n",
        "\n",
        "    # Create metadata for each chunk, storing the chunk index and source file name\n",
        "    metadata_list = [{\"chunk\": i, \"source\": file_name} for i in range(len(chunks))]\n",
        "\n",
        "    # Define batch size for inserting chunks to optimize performance\n",
        "    batch_size = 40\n",
        "\n",
        "    # Insert chunks into the database in batches\n",
        "    for i in range(0, len(chunks), batch_size):\n",
        "        end_id = min(i + batch_size, len(chunks))  # Ensure we don't exceed list length\n",
        "\n",
        "        # Add the batch of chunks to the vector store\n",
        "        db.add(\n",
        "            documents=chunks[i:end_id],\n",
        "            metadatas=metadata_list[i:end_id],\n",
        "            ids=id_list[i:end_id]\n",
        "        )\n",
        "\n",
        "    print(f\"{len(chunks)} chunks added to the vector store\")\n"
      ],
      "metadata": {
        "id": "Mz5u2hy-UeUm"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Retrieve Chunks**"
      ],
      "metadata": {
        "id": "duNeKQucZBnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, List\n",
        "\n",
        "def retrieve_chunks(db: Collection, query: str, n_results: int = 2) -> List[Any]:\n",
        "    \"\"\"\n",
        "    Retrieves relevant chunks from the  vector store for the given query.\n",
        "\n",
        "    Args:\n",
        "        db (Collection): The vector store object\n",
        "        query (str): The search query text.\n",
        "        n_results (int, optional): The number of relevant chunks to retrieve. Defaults to 2.\n",
        "\n",
        "    Returns:\n",
        "        List[Any]: A list of relevant chunks retrieved from the vector store.\n",
        "    \"\"\"\n",
        "\n",
        "    # Perform a query on the database to get the most relevant chunks\n",
        "    relevant_chunks = db.query(query_texts=[query], n_results=n_results)\n",
        "\n",
        "    # Return the retrieved relevant chunks\n",
        "    return relevant_chunks\n"
      ],
      "metadata": {
        "id": "OiLTV_9jZGCy"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Build Context**"
      ],
      "metadata": {
        "id": "arKLWO02ZrrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_context(relevant_chunks) -> str:\n",
        "    \"\"\"\n",
        "    Builds a single context string by combining texts from relevant chunks.\n",
        "\n",
        "    Args:\n",
        "        relevant_chunks: relevant chunks retrieved from the vector store.\n",
        "\n",
        "    Returns:\n",
        "        str: A single string containing all document chunks combined with newline separators.\n",
        "    \"\"\"\n",
        "\n",
        "    # combine the text from relevant chunks with newline separator\n",
        "    context = \"\\n\".join(relevant_chunks['documents'][0])\n",
        "\n",
        "    # Return the combined context string\n",
        "    return context\n"
      ],
      "metadata": {
        "id": "Re7I3y4iZwuz"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Build RAG Pipeline**"
      ],
      "metadata": {
        "id": "j-X0x6gtbp2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Tuple\n",
        "\n",
        "def get_context(pdf_path: str, query: str, db_path: str) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Retrieves the relevant chunks from the vector store and then builds context from them.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): The file path to the PDF document.\n",
        "        query (str): The query string to search within the vector store.\n",
        "        db_path (str): The file path to the persistent vector store database.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[str, str]: A tuple containing the context related to the query and the original query string.\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if the vector store already exists\n",
        "    if os.path.exists(db_path):\n",
        "        print(\"Loading existing vector store...\")\n",
        "\n",
        "        # Initialize the persistent client for the existing database\n",
        "        client = chromadb.PersistentClient(path=db_path)\n",
        "\n",
        "        # Create the embedding function using OpenAI embeddings\n",
        "        embeddings = embedding_functions.OpenAIEmbeddingFunction(\n",
        "            api_key=userdata.get(\"OPENAI_API_KEY\"),  # Fetch API key from userdata\n",
        "            model_name=\"text-embedding-3-small\"      # Specify the embedding model\n",
        "        )\n",
        "\n",
        "        # Get the collection of PDF chunks from the existing vector store\n",
        "        db = client.get_collection(name=\"pdf_chunks\", embedding_function=embeddings)\n",
        "    else:\n",
        "        print(\"Creating new vector store...\")\n",
        "\n",
        "        # Extract text from the provided PDF\n",
        "        pdf_text = text_extract(pdf_path)\n",
        "\n",
        "        # Chunk the extracted text\n",
        "        chunks = text_chunk(pdf_text)\n",
        "\n",
        "        # Create a new vector store\n",
        "        db = create_vector_store(db_path)\n",
        "\n",
        "        # Insert the text chunks into the vector store\n",
        "        insert_chunks_vectordb(chunks, db, pdf_path)\n",
        "\n",
        "    # Retrieve the relevant chunks based on the query\n",
        "    relevant_chunks = retrieve_chunks(db, query)\n",
        "\n",
        "    # Build the context from the relevant chunks\n",
        "    context = build_context(relevant_chunks)\n",
        "\n",
        "    # Return the context and the original query\n",
        "    return context, query\n"
      ],
      "metadata": {
        "id": "azZAY-BraBwf"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_prompt(context: str, query: str) -> str:\n",
        "    \"\"\"\n",
        "    Generates a rag prompt based on the given context and query.\n",
        "\n",
        "    Args:\n",
        "        context (str): The context the LLM should use to answer the question.\n",
        "        query (str): The user query that needs to be answered based on the context.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated rag prompt.\n",
        "    \"\"\"\n",
        "\n",
        "    # Format the prompt with the provided context and query\n",
        "    rag_prompt = f\"\"\" You are an AI model trained for question answering. You should answer the\n",
        "    given question based on the given context only.\n",
        "    Question : {query}\n",
        "    \\n\n",
        "    Context : {context}\n",
        "    \\n\n",
        "    If the answer is not present in the given context, respond as: The answer to this question is not available\n",
        "    in the provided content.\n",
        "    \"\"\"\n",
        "\n",
        "    # Return the formatted prompt\n",
        "    return rag_prompt\n"
      ],
      "metadata": {
        "id": "gzAC5TswbrnO"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from litellm import completion\n",
        "\n",
        "def get_response(rag_prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Sends a prompt to the OpenAI LLM and returns the answer.\n",
        "\n",
        "    Args:\n",
        "        rag_prompt (str): The rag prompt.\n",
        "\n",
        "    Returns:\n",
        "        str: The LLM generated answer.\n",
        "    \"\"\"\n",
        "    # Specify the LLM to use\n",
        "    model = \"openai/gpt-4o-mini\"\n",
        "\n",
        "    # Prepare the message to be sent to the model\n",
        "    messages = [{\"role\": \"user\", \"content\": rag_prompt}]\n",
        "\n",
        "    # Call the completion function to get a response from the model\n",
        "    response = completion(model=model, messages=messages, temperature=0)\n",
        "\n",
        "    # Return the answer\n",
        "    answer = response.choices[0].message.content\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "8LxkJY_gcd5g"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def rag_pipeline(pdf_path: str, query: str, db_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Runs a Retrieval-Augmented Generation (RAG) pipeline to retrieve context from a vector store,\n",
        "    generate the rag prompt, and then get the answer from the model.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): The file path to the PDF document from which context is extracted.\n",
        "        query (str): The query for which a response is needed, based on the context.\n",
        "        db_path (str): The file path to the persistent vector store database used for context retrieval.\n",
        "\n",
        "    Returns:\n",
        "        str: The model's response based on the context and the provided query.\n",
        "    \"\"\"\n",
        "\n",
        "    # get the context\n",
        "    context, query = get_context(pdf_path, query, db_path)\n",
        "\n",
        "    # Generate the rag prompt based on the context and query\n",
        "    rag_prompt = get_prompt(context, query)\n",
        "\n",
        "    # Get the response from the model using the rag prompt\n",
        "    response = get_response(rag_prompt)\n",
        "\n",
        "    # Return the model's response\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "GQLzNtoWd3pd"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Run RAG Pipeline**"
      ],
      "metadata": {
        "id": "LdIxhjaBn6d4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the chroma DB path\n",
        "current_dir = \"/content/rag\"\n",
        "persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db_pdf\")\n",
        "\n",
        "# PDF path\n",
        "pdf_path = \"/content/attention_is_all_you_need.pdf\"\n",
        "\n",
        "# RAG query\n",
        "query = \"What is the transformer architecture?\""
      ],
      "metadata": {
        "id": "zyAPVswXegm8"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the RAG pipeline\n",
        "answer = rag_pipeline(pdf_path, query, persistent_directory)"
      ],
      "metadata": {
        "id": "bZcKzayselVs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "a08906b1-87b5-4521-9638-6a5f182b937a"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading existing vector store...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AuthenticationError",
          "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-nEnLz***************************************jjkb. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}} in query.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2477387613.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run the RAG pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrag_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpersistent_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-185521177.py\u001b[0m in \u001b[0;36mrag_pipeline\u001b[0;34m(pdf_path, query, db_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# get the context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Generate the rag prompt based on the context and query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2282302727.py\u001b[0m in \u001b[0;36mget_context\u001b[0;34m(pdf_path, query, db_path)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Retrieve the relevant chunks based on the query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mrelevant_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieve_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Build the context from the relevant chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-918405536.py\u001b[0m in \u001b[0;36mretrieve_chunks\u001b[0;34m(db, query, n_results)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Perform a query on the database to get the most relevant chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mrelevant_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_texts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Return the retrieved relevant chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/models/Collection.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, query_embeddings, query_texts, query_images, query_uris, ids, n_results, where, where_document, include)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \"\"\"\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         query_request = self._validate_and_prepare_query_request(\n\u001b[0m\u001b[1;32m    210\u001b[0m             \u001b[0mquery_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mquery_texts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_texts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/models/CollectionCommon.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{str(e)} in {name}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/models/CollectionCommon.py\u001b[0m in \u001b[0;36m_validate_and_prepare_query_request\u001b[0;34m(self, query_embeddings, query_texts, query_images, query_uris, ids, n_results, where, where_document, include)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mquery_records\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"embeddings\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0mvalidate_record_set_for_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_records\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mrequest_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embed_record_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_records\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mrequest_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_records\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"embeddings\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/models/CollectionCommon.py\u001b[0m in \u001b[0;36m_embed_record_set\u001b[0;34m(self, record_set, embeddable_fields)\u001b[0m\n\u001b[1;32m    549\u001b[0m                     )\n\u001b[1;32m    550\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecord_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[literal-required]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m         raise ValueError(\n\u001b[1;32m    553\u001b[0m             \u001b[0;34m\"Record does not contain any non-None fields that can be embedded.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/models/CollectionCommon.py\u001b[0m in \u001b[0;36m_embed\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDefaultEmbeddingFunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         ):\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0mconfig_ef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"embedding_function\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig_ef\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/types.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mEmbeddingFunction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mEmbeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalidate_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/utils/embedding_functions/openai_embedding_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# Get embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0membedding_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# Extract embeddings from response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/embeddings.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0;34m\"/embeddings\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaybe_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_create_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbeddingCreateParams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-nEnLz***************************************jjkb. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}} in query."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Query:{query}\")\n",
        "print(f\"Generated answer:{answer}\")"
      ],
      "metadata": {
        "id": "djQeTMMNeoDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG query\n",
        "query = \"What is self-attention?\"\n",
        "\n",
        "# Run the RAG pipeline\n",
        "answer = rag_pipeline(pdf_path, query, persistent_directory)\n",
        "\n",
        "print(f\"Query:{query}\")\n",
        "print(f\"Generated answer:{answer}\")"
      ],
      "metadata": {
        "id": "_XJ4ulaInq48"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}